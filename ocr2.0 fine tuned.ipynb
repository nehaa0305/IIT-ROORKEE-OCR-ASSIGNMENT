{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d66760f-5799-4671-8f8a-4b9fa3d5b6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images shape: torch.Size([32, 1, 128, 128])\n",
      "Batch of labels: tensor([ 7,  1,  7, 19,  4, 33, 33, 19, 16, 26, 32, 22, 17, 28, 28, 13,  4,  6,\n",
      "         1,  7,  9,  1,  5, 31, 25,  4, 16, 16, 31,  2,  7, 18])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class DevanagariDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, max_images_per_character=100):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.max_images_per_character = max_images_per_character\n",
    "        self.load_images()\n",
    "\n",
    "    def load_images(self):\n",
    "        for outer_folder_name in os.listdir(self.root_dir):\n",
    "            outer_folder_path = os.path.join(self.root_dir, outer_folder_name)\n",
    "            if os.path.isdir(outer_folder_path):\n",
    "                for folder_name in os.listdir(outer_folder_path):\n",
    "                    folder_path = os.path.join(outer_folder_path, folder_name)\n",
    "                    label = folder_name.split('_')[1]  \n",
    "                    if os.path.isdir(folder_path):\n",
    "                        # Limit the number of images loaded per character\n",
    "                        images_loaded = 0\n",
    "                        for img_name in os.listdir(folder_path):\n",
    "                            img_path = os.path.join(folder_path, img_name)\n",
    "                            self.image_paths.append(img_path)\n",
    "                            self.labels.append(int(label))  \n",
    "                            images_loaded += 1\n",
    "                            if images_loaded >= self.max_images_per_character:\n",
    "                                break  # Stop loading more images for this character\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path).convert(\"L\")  \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Define transformations for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  \n",
    "])\n",
    "\n",
    "# Create the dataset with a limit on the number of images per character\n",
    "train_dataset = DevanagariDataset(\n",
    "    root_dir=r'C:\\Users\\asus\\Desktop\\chatbot\\IIT ROORKEE\\Train', \n",
    "    transform=transform,\n",
    "    max_images_per_character=10  # Limit to 10 images per character\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Check the shape of images and labels in the first batch\n",
    "for images, labels in train_loader:\n",
    "    print(\"Batch of images shape:\", images.shape)\n",
    "    print(\"Batch of labels:\", labels)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f004bc35-355c-496a-8e80-986f8dfa5e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GOTQwenForCausalLM(\n",
       "  (model): GOTQwenModel(\n",
       "    (embed_tokens): Embedding(151860, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=2816, bias=False)\n",
       "          (down_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "    (vision_tower_high): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "      (net_2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (net_3): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (mm_projector_vary): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=151860, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")  # or \"cuda\" if you want to use GPU\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df16370-0a3e-4f05-8df1-ccf86852f073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True, low_cpu_mem_usage=True)\n",
    "model = model.eval().cuda()\n",
    "device = torch.device(\"cpu\")  # or \"cuda\" if you want to use GPU\n",
    "model.to(device)\n",
    "\n",
    "num_classes = 46  \n",
    "model.classifier = nn.Linear(model.config.hidden_size, num_classes).cuda()\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00cb88cd-6407-4f85-8fa5-6bac56ef6069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktokenNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading tiktoken-0.7.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
      "Downloading tiktoken-0.7.0-cp311-cp311-win_amd64.whl (799 kB)\n",
      "   ---------------------------------------- 0.0/799.0 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 184.3/799.0 kB 5.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 409.6/799.0 kB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 624.6/799.0 kB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 799.0/799.0 kB 4.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b4f9dae-1205-4dbc-a0bf-cf54812febd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting verovioNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading verovio-4.3.1-cp311-cp311-win_amd64.whl.metadata (9.2 kB)\n",
      "Downloading verovio-4.3.1-cp311-cp311-win_amd64.whl (4.8 MB)\n",
      "   ---------------------------------------- 0.0/4.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/4.8 MB 5.0 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.4/4.8 MB 4.6 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.6/4.8 MB 4.5 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.8/4.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.8/4.8 MB 4.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.8/4.8 MB 3.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.0/4.8 MB 3.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.0/4.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.0/4.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.0/4.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.0/4.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.0/4.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.0/4.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.0/4.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.0/4.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.0/4.8 MB 2.8 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.0/4.8 MB 2.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.2/4.8 MB 1.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 1.5/4.8 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 1.7/4.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.0/4.8 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.2/4.8 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.6/4.8 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.9/4.8 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.1/4.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.1/4.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.1/4.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.1/4.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.1/4.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.1/4.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.1/4.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.1/4.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.1/4.8 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.3/4.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 3.7/4.8 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.1/4.8 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 4.4/4.8 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 4.7/4.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.8/4.8 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.8/4.8 MB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: verovio\n",
      "Successfully installed verovio-4.3.1\n"
     ]
    }
   ],
   "source": [
    "%pip install verovio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d10d3-c0e1-4644-b97b-c4a4df768b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-gpt",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
